{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this file, we will conduct all of our tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nbimporter in /home/jupyter-smitherr/.local/lib/python3.9/site-packages (0.3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from dgl.dataloading.pytorch import GraphDataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import nbimporter\n",
    "import dataset as ds\n",
    "import model as mfile\n",
    "from score import test\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ds.SyntheticDataset()\n",
    "batch_size = 1\n",
    "\n",
    "# We want batch size to be 1 because do not want batched graphs (as this is not the correct structure of our individual molecules)\n",
    "train_dataloader = GraphDataLoader(train_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from os.path import exists\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def train(model, epochs, file_name='SavedModels/rj_electron.pth', output=False, debug_batch_interval=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.00005)\n",
    "    \n",
    "    # Try to load best_mae\n",
    "    best_mae = None\n",
    "    if exists('SavedModels/bestmae.txt'):\n",
    "        with open('SavedModels/bestmae.txt', 'r') as f:\n",
    "            best_mae = float(f.read())\n",
    "            print(\"Looking to beat best MAE of\", best_mae)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs), position=0, desc=\"Epochs\"):\n",
    "        \n",
    "        running, batch_running, ct, batch_ct = 0, 0, 0, 0\n",
    "        print('Epoch', epoch+1)\n",
    "        for batch_idx, (graph, label) in tqdm(enumerate(train_dataloader), position=1, desc=\"Batches\", total=len(train_dataloader) * batch_size):\n",
    "            # Some labels may be none (RDKit errors), so move on\n",
    "            # Ideally this has been filtered out by this step, but as of now there are still some in the dataset\n",
    "            if np.isnan(label):\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            bf = graph.edata['bond_feats'].float()\n",
    "            af = graph.ndata['atom_feats'].float()\n",
    "            \n",
    "            # Not sure about an error we are experiencing. This is occurring towards the end, so we will just ignore this 1/1000 sample\n",
    "            try:\n",
    "                y_pred = model(graph, af, bf)[0]\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            # The 23.06 is the same value used in score.py (conversion to kcal/mol)\n",
    "            # L1 is MAE, L2 is MSE\n",
    "            loss = torch.nn.functional.l1_loss(y_pred.reshape(1), label) * 23.06 # ((y_pred.reshape(1,-1) - batch_y)**2).sum()\n",
    "            running += loss.item()\n",
    "            batch_running += loss.item()\n",
    "            ct += 1\n",
    "            batch_ct += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                     \n",
    "            # Every debug_batch_interval iterations, print the data we've churned through (iterations * data per batch)\n",
    "            if output and batch_idx % (len(train_dataloader) // debug_batch_interval) == 0:                \n",
    "                print('Epoch: {} [{}/{} ({:.0f}%)]\\tBatch Loss: {:.2f}\\tEpoch Loss: {:.2f}'.format(\n",
    "                          epoch+1, batch_idx, len(train_dataloader) * batch_size,    # current sample num / total num\n",
    "                          100. * batch_idx / len(train_dataloader), # this batch num's % of total dataset\n",
    "                          batch_running // batch_ct, # the loss for this batch\n",
    "                          running // ct) # running loss for the epoch\n",
    "                     )\n",
    "                batch_running, batch_ct = 0, 0\n",
    "                \n",
    "        this_loss = running / ct\n",
    "        if output:\n",
    "            print(\"\\nAverage Loss:\", round(running / ct * 100) / 100.0,\"\\n\")\n",
    "        else:\n",
    "            print(\"Epoch\", epoch+1, \"Average Loss:\", round(this_loss * 100) / 100.0)\n",
    "            \n",
    "        # Save our model\n",
    "        if not best_mae:\n",
    "            best_mae = this_loss\n",
    "            checkpoint = {'state_dict': model.state_dict(),'optimizer': optimizer.state_dict()}\n",
    "            torch.save(checkpoint, file_name)\n",
    "        if this_loss < best_mae:\n",
    "            best_mae = this_loss\n",
    "            print(\"New best model found! Saving with loss of\", best_mae)\n",
    "            \n",
    "            # Write our best mae so we can keep track every time we retrain\n",
    "            with open('SavedModels/bestmae.txt', 'w') as f:\n",
    "                f.write(str(best_mae))\n",
    "            checkpoint = {'state_dict': model.state_dict(),'optimizer': optimizer.state_dict()}\n",
    "            torch.save(checkpoint, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 16 (node), 5 (edge)\n"
     ]
    }
   ],
   "source": [
    "# All graphs in the list have the same scheme size, so pull the dimensions from the first\n",
    "node_dim = train_dataset[0][0].ndata['atom_feats'].shape[1]\n",
    "edge_dim = train_dataset[0][0].edata['bond_feats'].shape[1]\n",
    "print(\"Dimensions:\", node_dim, \"(node),\", edge_dim, \"(edge)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgllife\n",
    "model = mfile.Electron_Predictor_3k(node_dim, edge_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"SavedModels/4k_ish_rj_electron.pth\")[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Description: \\\n",
    "Our model follows a similar architecture as the MPNN model. It consists of a two linear layers (one at the front, one at the end), a convolution layer, and a GRU layer.\n",
    "\n",
    "- **fc1**: This linear + relu is our first \"line of attack,\" looking for connectings between our data before we lose information on individual atoms via convolution. It also projects our data to a higher dimension. I talk about tall vs wide as we learned from class in our model file\n",
    "- **gnn1**: This layer uses convolution involving two hidden layers to try and grab information about neighbors in an efficient manner\n",
    "- **gru**: To be completely honest, I am not entirely sure I understand GRUs. My only understanding of it is that it serves to eliminate the issue of the vanishing gradient which we could expect to stumble upon after our fc1 and gnn layers. We are experimented with getting rid of it, and we saw that convergence was decelerating quicker than with it... so we keep it!\n",
    "- **gnn2**: This layer is a different flavor of graph convolution. The GatedGraphConv was referenced in the paper on MPNNs (https://arxiv.org/pdf/1704.01212.pdf) before they switched to NNConv. Ignoring the time complexity issues (which we serve to rectify by eliminating many of our useless features (quick side note, we also did a bit of \"brain-surgery\" on our network to try to trace the least important features)), GGC had some positives to it. I figure that reintroducing it after an NNConv might provide another, differing convolution which could be a valuable composition to feed to our readout and prediction functions.\n",
    "- **gru**: Reusing the same GRU layer before, keeps the gradient after gnn2\n",
    "- ~~**fc2**: This fully-connected layer serves as our final decision maker, projecting back into 1 dimension (granted there is only 1 dimension at this point anyways) and trying to making sense of the previously convoluted data~~ Removed this as of 13 Nov 21 since we incorporated the MPNN readout and prediction which perform much better. Also an fc2 at the end caused a linear regression on the entire thing and caused the model to predict the average output pe\n",
    "\n",
    "The first investigation into our important modifications of this model which differentiates it from the MPNN model stems from the negative min_PE output labels. To combat our model giving large error from positive results, many Relu's were stripped from the model, both in the architecture itself and in the forward passes. I experimented with a linear \"decision\" layer at the very end, but this caused the model to try to make an approximation of the output labels which would end up with the average of the output labels (minimizing error with a constant). As you can imagine, this is unideal, so we ended up scrapping this idea. I also later realized that a prediction layer with negative weights could easily bypass this entire concern.\n",
    "\n",
    "Training Description: \\\n",
    "To train, I have found that after about 8 epochs, the model begins to stablize. So, the training scheme is planned as follows:\n",
    "\n",
    "- 3 epochs w/ Adam opt @ 0.1\n",
    "- 3 epochs w/ Adam opt @ 0.01\n",
    "\n",
    "This is to help refine the smaller details of the gradient with respect to the weights in our model. This is essentially our own version of momentum because we try to have the model drop mae rather quickly, and then be refined with minute changes in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electron_Predictor(\n",
      "  (gnn): Electron_MPNN(\n",
      "    (fc1): Sequential(\n",
      "      (0): Linear(in_features=10, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (gnn1): NNConv(\n",
      "      (edge_func): Sequential(\n",
      "        (0): Linear(in_features=5, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=256, out_features=16384, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (gru): GRU(128, 128)\n",
      "  )\n",
      "  (predict): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking to beat best MAE of 3293.649390840706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cfab366998407781e456f8fcb92de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0476cacd6746c3b30f79990eba7764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [0/13478 (0%)]\tBatch Loss: 3344.00\tEpoch Loss: 3344.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24594/3293130170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_24594/4065303503.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, file_name, output, debug_batch_interval)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mbatch_ct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# Every debug_batch_interval iterations, print the data we've churned through (iterations * data per batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, 15, output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to keep jupyterlab running even when computer dies/goes to sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture stored_output\n",
    "\n",
    "# Upon returning, call\n",
    "# stored_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output for train on MPNN prebuilt\n",
    "'''\n",
    "Epoch: 1 [0/13480 (0%)]\tBatch Loss: 15816.00\tEpoch Loss: 15816.00\n",
    "Epoch: 1 [1348/13480 (10%)]\tBatch Loss: 21889.00\tEpoch Loss: 21884.00\n",
    "Epoch: 1 [2696/13480 (20%)]\tBatch Loss: 21315.00\tEpoch Loss: 21600.00\n",
    "Epoch: 1 [4044/13480 (30%)]\tBatch Loss: 20245.00\tEpoch Loss: 21148.00\n",
    "Epoch: 1 [5392/13480 (40%)]\tBatch Loss: 18982.00\tEpoch Loss: 20607.0\n",
    "'''\n",
    "\n",
    "# Output for train on custom MPNN model (note I upped the log interval... so this 29k error is with only 132 samples)\n",
    "'''\n",
    "Epoch: 1 [0/13480 (0%)]\tBatch Loss: 36117.00\tEpoch Loss: 36117.00\n",
    "Epoch: 1 [44/13480 (0%)]\tBatch Loss: 43399.00\tEpoch Loss: 43237.00\n",
    "Epoch: 1 [88/13480 (1%)]\tBatch Loss: 22759.00\tEpoch Loss: 33113.00\n",
    "Epoch: 1 [132/13480 (1%)]\tBatch Loss: 20600.00\tEpoch Loss: 28974.00\n",
    "'''\n",
    "\n",
    "# Output for train on custom MPNN (with high-speed discrete graphics card)\n",
    "'''\n",
    "Epoch: 1 [0/13478 (0%)]\tBatch Loss: 49005.00\tEpoch Loss: 49005.00\n",
    "Epoch: 1 [2695/13478 (20%)]\tBatch Loss: 34606.00\tEpoch Loss: 34611.00\n",
    "Epoch: 1 [5390/13478 (40%)]\tBatch Loss: 19776.00\tEpoch Loss: 27195.00\n",
    "Epoch: 1 [8085/13478 (60%)]\tBatch Loss: 19633.00\tEpoch Loss: 24675.00\n",
    "Epoch: 1 [10780/13478 (80%)]\tBatch Loss: 19374.00\tEpoch Loss: 23350.00\n",
    "Epoch: 1 [13475/13478 (100%)]\tBatch Loss: 18702.00\tEpoch Loss: 22420.00\n",
    "Average Loss: 22420.12 \n",
    "\n",
    "...\n",
    "\n",
    "Epoch: 5 [0/13478 (0%)]\tBatch Loss: 148.00\tEpoch Loss: 148.00\n",
    "Epoch: 5 [2695/13478 (20%)]\tBatch Loss: 11514.00\tEpoch Loss: 11510.00\n",
    "Epoch: 5 [5390/13478 (40%)]\tBatch Loss: 11640.00\tEpoch Loss: 11575.00\n",
    "Epoch: 5 [8085/13478 (60%)]\tBatch Loss: 11229.00\tEpoch Loss: 11459.00\n",
    "Epoch: 5 [10780/13478 (80%)]\tBatch Loss: 11225.00\tEpoch Loss: 11401.00\n",
    "Epoch: 5 [13475/13478 (100%)]\tBatch Loss: 10779.00\tEpoch Loss: 11276.00\n",
    "Average Loss: 11276.78\n",
    "\n",
    "...\n",
    "\n",
    "Epoch: 10 [0/13478 (0%)]\tBatch Loss: 4457.00\tEpoch Loss: 4457.00\n",
    "Epoch: 10 [2695/13478 (20%)]\tBatch Loss: 7007.00\tEpoch Loss: 7006.00\n",
    "Epoch: 10 [5390/13478 (40%)]\tBatch Loss: 6615.00\tEpoch Loss: 6811.00\n",
    "Epoch: 10 [8085/13478 (60%)]\tBatch Loss: 6603.00\tEpoch Loss: 6741.00\n",
    "Epoch: 10 [10780/13478 (80%)]\tBatch Loss: 6489.00\tEpoch Loss: 6678.00\n",
    "Epoch: 10 [13475/13478 (100%)]\tBatch Loss: 6101.00\tEpoch Loss: 6563.00\n",
    "Average Loss: 6562.86 \n",
    "\n",
    "...\n",
    "\n",
    "Epoch: 15 [0/13478 (0%)]\tBatch Loss: 14334.00\tEpoch Loss: 14334.00\n",
    "Epoch: 15 [2695/13478 (20%)]\tBatch Loss: 4727.00\tEpoch Loss: 4731.00\n",
    "Epoch: 15 [5390/13478 (40%)]\tBatch Loss: 4701.00\tEpoch Loss: 4716.00\n",
    "Epoch: 15 [8085/13478 (60%)]\tBatch Loss: 4882.00\tEpoch Loss: 4771.00\n",
    "Epoch: 15 [10780/13478 (80%)]\tBatch Loss: 4562.00\tEpoch Loss: 4719.00\n",
    "Epoch: 15 [13475/13478 (100%)]\tBatch Loss: 4519.00\tEpoch Loss: 4679.00\n",
    "Average Loss: 4678.87 \n",
    "\n",
    "...\n",
    "\n",
    "Epoch: 20 [0/13478 (0%)]\tBatch Loss: 4333.00\tEpoch Loss: 4333.00\n",
    "Epoch: 20 [2695/13478 (20%)]\tBatch Loss: 3833.00\tEpoch Loss: 3833.00\n",
    "Epoch: 20 [5390/13478 (40%)]\tBatch Loss: 3955.00\tEpoch Loss: 3894.00\n",
    "Epoch: 20 [8085/13478 (60%)]\tBatch Loss: 3969.00\tEpoch Loss: 3919.00\n",
    "Epoch: 20 [10780/13478 (80%)]\tBatch Loss: 3634.00\tEpoch Loss: 3848.00\n",
    "Epoch: 20 [13475/13478 (100%)]\tBatch Loss: 3888.00\tEpoch Loss: 3856.00\n",
    "Average Loss: 3856.48 \n",
    "\n",
    "'''\n",
    "\n",
    "# Output for train on reduced features (w/ energy feaature)\n",
    "'''\n",
    "Epoch: 1 [0/13478 (0%)]\tBatch Loss: 44627.00\tEpoch Loss: 44627.00\n",
    "Epoch: 1 [2695/13478 (20%)]\tBatch Loss: 33025.00\tEpoch Loss: 33030.00\n",
    "Epoch: 1 [5390/13478 (40%)]\tBatch Loss: 22461.00\tEpoch Loss: 27747.00\n",
    "Epoch: 1 [8085/13478 (60%)]\tBatch Loss: 20027.00\tEpoch Loss: 25174.00\n",
    "Epoch: 1 [10780/13478 (80%)]\tBatch Loss: 16405.00\tEpoch Loss: 22982.00\n",
    "Epoch: 1 [13475/13478 (100%)]\tBatch Loss: 12521.00\tEpoch Loss: 20890.00\n",
    "Average Loss: 20891.38 \n",
    "\n",
    "Epoch: 2 [0/13478 (0%)]\tBatch Loss: 3083.00\tEpoch Loss: 3083.00\n",
    "Epoch: 2 [2695/13478 (20%)]\tBatch Loss: 11075.00\tEpoch Loss: 11072.00\n",
    "Epoch: 2 [5390/13478 (40%)]\tBatch Loss: 10140.00\tEpoch Loss: 10606.00\n",
    "Epoch: 2 [8085/13478 (60%)]\tBatch Loss: 9159.00\tEpoch Loss: 10124.00\n",
    "Epoch: 2 [10780/13478 (80%)]\tBatch Loss: 8855.00\tEpoch Loss: 9807.00\n",
    "Epoch: 2 [13475/13478 (100%)]\tBatch Loss: 8362.00\tEpoch Loss: 9518.00\n",
    "Average Loss: 9517.3 \n",
    "\n",
    "(capped around 5k)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE TO RONAN \n",
    "Save model with code below (keep in mind auto-saving only happens when an epoch completes (and if the loss is below our minimum loss of any model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'state_dict': model.state_dict()}\n",
    "torch.save(checkpoint, \"SavedModels/4k_ish_rj_electron.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2519.398824028712"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, \"Electron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources and References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://arxiv.org/pdf/1704.01212.pdf\n",
    "2. https://arxiv.org/pdf/1806.03146.pdf\n",
    "3. https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/60c7579dbb8c1a48b63dc892/original/a-graph-neural-network-for-predicting-energy-and-stability-of-known-and-hypothetical-crystal-structures.pdf#page=11&zoom=100,76,125\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
