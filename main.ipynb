{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rdkit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8c785bad9f3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeaturize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Desktop\\Projects\\Competitions\\PredictingSolubility\\score.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmordred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmordred\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCalculator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescriptors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rdkit'"
     ]
    }
   ],
   "source": [
    "from score import featurize, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mordred.AtomCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    mordred.AtomCount.AtomCount('C'),\n",
    "    mordred.AtomCount.AtomCount('H'),\n",
    "    mordred.AtomCount.AtomCount('O'),\n",
    "    mordred.AtomCount.AtomCount('N'),\n",
    "\n",
    "#     mordred.CPSA.FPSA(5),\n",
    "#     mordred.CPSA.PPSA(5),\n",
    "#     mordred.CPSA.FNSA(5),\n",
    "#     mordred.CPSA.PNSA(5),\n",
    "#     mordred.CPSA.DPSA(5),   # Would like to see if difference in charged partial surface area will cover the previous 4 features\n",
    "    \n",
    "    mordred.CarbonTypes.CarbonTypes(1,2),\n",
    "    mordred.HydrogenBond.HBondAcceptor(),\n",
    "    mordred.HydrogenBond.HBondDonor(),\n",
    "    mordred.TopoPSA.TopoPSA(False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1210/1210 [00:02<00:00, 428.61it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = featurize('Data/Solubility/dataset-F.csv',features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1291/1291 [00:02<00:00, 444.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.166639528565129"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(features,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Electron\n",
    "We have decided to include our comments on this notebook as sort of a \"storytelling\" presentation. Feel free to read our comments as you go!\n",
    "\n",
    "## Background\n",
    "\n",
    "This section begins with some of the analysis I found from homework three. First of all, by simply trial and error of many different models (NO FREE LUNCH), I found that the validation error was able to be generalized to 0.6 for the Decision Tree Regressor model. This does not mean we will choose this model in our final submission, but we will begin testing there and looking at similar alternatives.\n",
    "\n",
    "One of the observations I noted was that the difference between the validation and training errors were not yet converging. That is, the difference was still consistently decreasing even at 100% of the dataset. At the same time, there were no noticeable jitters/extra noise around either the training or the validation datasets, which indicates that the model has the potential to be even more accurate.\n",
    "\n",
    "The primary limiting factor then (ignoring parameter optimization) is the amount of data. I would hypothesize that with more data, we could optimize this model. Luckily for us, Prof. Tristan gave us multiple datasets to work with. Thus, step one will be preparing our data by combining our datasets. This way, we can take advantage of as much data as possible. Analytically, this only has benefits (will do no harm) as determined in the Hoeffding Inequality.\n",
    "\n",
    "Let's get started! :)\n",
    "-RJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Copy over some useful functions from HW 3\n",
    "def get_pipeline(models, degree):\n",
    "    model_tups = [('model-0', PolynomialFeatures(degree))]\n",
    "    for i in range(len(models)):\n",
    "        model_tups.append(('model-'+str(i+1), models[i]))\n",
    "    return Pipeline(model_tups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dataset-A.csv with dim (6110, 6)\n",
      "Added dataset-B.csv with dim (4650, 6)\n",
      "Added dataset-C.csv with dim (2603, 6)\n",
      "Added dataset-D.csv with dim (2115, 6)\n",
      "Added dataset-F.csv with dim (1210, 6)\n",
      "Added dataset-G.csv with dim (1144, 6)\n",
      "Added dataset-H.csv with dim (578, 6)\n",
      "Added dataset-I.csv with dim (94, 6)\n",
      "Our combined dataset has 18504 rows with 6 features.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Combine Datasets\n",
    "def generate_combined_dataset(dir_path, fnames, out_name=\"\", debug=False):\n",
    "    combined = None\n",
    "    for f in fnames:\n",
    "        df = pd.read_csv(dir_path + f)\n",
    "        if combined is None:\n",
    "            combined = df.copy()\n",
    "        else:\n",
    "            ## TODO: Check that \n",
    "            combined = pd.concat([combined, df])\n",
    "        if debug:\n",
    "            print(\"Added \" + str(f) + \" with dim\", df.shape)\n",
    "    if debug:\n",
    "        print(\"Our combined dataset has\", combined.shape[0], \"rows with\", combined.shape[1], \"features.\")\n",
    "    return combined\n",
    "    \n",
    "# Restriction: \"It is forbidden to use any dataset other than A, B, C, D, F, G, H, I\"\n",
    "# So, we make sure we only use these datasets in our data preparation\n",
    "generate_combined_dataset(\"../../Data/Solubility/\",\n",
    "                          [\"dataset-A.csv\", \"dataset-B.csv\", \"dataset-C.csv\",\n",
    "                          \"dataset-D.csv\", \"dataset-F.csv\", \"dataset-G.csv\",\n",
    "                          \"dataset-H.csv\", \"dataset-I.csv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "print(\"----- DECISION TREE REGRESSOR -----\")\n",
    "# Needs more data (would combine datasets to achieve this) or should reduce complexity\n",
    "models = [DecisionTreeRegressor()]\n",
    "validation_curve_poly_degree(X_train, y_train, X_test, y_test, models=models)\n",
    "learning_curves_data_used(X_train, y_train, X_test, y_test, models=models)\n",
    "\n",
    "pl = Pipeline([('poly', PolynomialFeatures()), ('pca', PCA()), ('ridge', Ridge())])\n",
    "\n",
    "# Center training X\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_cent = scaler.transform(X_train)\n",
    "\n",
    "# Center testing X\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_test)\n",
    "X_test_cent = scaler.transform(X_test)\n",
    "\n",
    "parameters = {'poly__degree': [1,2,3], 'pca__n_components': [2,3,4,5,9]}\n",
    "gscv = GridSearchCV(pl, parameters)\n",
    "gscv.fit(X_train_cent, y_train)\n",
    "# print(gscv.cv_results_)\n",
    "# gscv is best model\n",
    "\n",
    "def print_best_model(grid_search_model):\n",
    "    ranks = grid_search_model.cv_results_[\"rank_test_score\"]\n",
    "    best_index = np.where(ranks==1)[0][0]\n",
    "    print(\"The best model had parameters:\", grid_search_model.cv_results_[\"params\"][best_index])\n",
    "\n",
    "print_best_model(gscv)\n",
    "mean_absolute_error(y_test, gscv.predict(X_test_cent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
